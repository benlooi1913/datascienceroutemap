{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDS503: Machine Learning\n",
    "\n",
    "***\n",
    "## LAB 2: K-Nearest Neighbour (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## A Simple Classification (K-Nearest Neighbour) using Python\n",
    "\n",
    "We are going to start demonstrating Python with a simple classification task. In this lab, we are going to explore the Cencus Income data set, It contains the cencus income of the people. They are trying to see the income of more than 50k and less than 50k. You should make a folder in the lab or your own computer to save data and your own work. This data was extracted from the census bureau database found at: http://www.census.gov/ftp/pub/DES/www/welcome.html.\n",
    "        \n",
    "\n",
    "In this Lab we are going to use the CRISP-DM model.\n",
    "<img src=\"CRISP-DM.jpg\" style=\"width:60%;margin-left:auto;margin-right:auto;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step1: Business Understanding\n",
    "\n",
    "***\n",
    "\n",
    "This data was extracted from the census bureau database found at: http://www.census.gov/ftp/pub/DES/www/welcome.html. It contains the cencus income of the people. They are trying to see the income of more than 50k and less than 50k.\n",
    "| Class label '>50K'  : 23.93% / 24.78% (without unknowns)\n",
    "| Class label '<=50K' : 76.07% / 75.22% (without unknowns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Part 2.png\" style=\"width:80%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "### Step 2: Data Understanding\n",
    "\n",
    "#### Description of the data:\n",
    "In the censuc income data set, there are fifteen attributes including the class attribute indicating the class/category information. \n",
    "The 15 attributes are:\n",
    "-  age: continuous.\n",
    "-  workclass:\n",
    "    - Private\n",
    "    - Self-emp-not-inc\n",
    "    - Self-emp-inc\n",
    "    - Federal-gov\n",
    "    - Local-gov\n",
    "    - State-gov\n",
    "    - Without-pay\n",
    "    - Never-worked\n",
    "- fnlwgt: continuous.\n",
    "- education: \n",
    "    - Bachelors \n",
    "    - Some-college\n",
    "    - 11th\n",
    "    - HS-grad\n",
    "    - Prof-school\n",
    "    - Assoc-acdm\n",
    "    - Assoc-voc\n",
    "    - 9th\n",
    "    - 7th-8th\n",
    "    - 12th\n",
    "    - Masters\n",
    "    - 1st-4th\n",
    "    - 10th\n",
    "    - Doctorate\n",
    "    - 5th-6th\n",
    "    - Preschool\n",
    "- education-num: continuous.\n",
    "- marital-status: \n",
    "    - Married-civ-spouse\n",
    "    - Divorced\n",
    "    - Never-married\n",
    "    - Separated\n",
    "    - Widowed\n",
    "    - Married-spouse-absent\n",
    "    - Married-AF-spouse\n",
    "- occupation: \n",
    "    - Tech-support\n",
    "    - Craft-repair\n",
    "    - Other-service\n",
    "    - Sales\n",
    "    - Exec-managerial\n",
    "    - Prof-specialty\n",
    "    - Handlers-cleaners\n",
    "    - Machine-op-inspct\n",
    "    - Adm-clerical\n",
    "    - Farming-fishing\n",
    "    - Transport-moving\n",
    "    - Priv-house-serv\n",
    "    - Protective-serv\n",
    "    - Armed-Forces\n",
    "- relationship: \n",
    "    - Wife\n",
    "    - Own-child\n",
    "    - Husband\n",
    "    - Not-in-family\n",
    "    - Other-relative\n",
    "    - Unmarried\n",
    "- race: \n",
    "    - White\n",
    "    - Asian-Pac-Islander\n",
    "    - Amer-Indian-Eskimo\n",
    "    - Other\n",
    "    - Black\n",
    "- sex: \n",
    "    - Female\n",
    "    - Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous.\n",
    "- native-country: \n",
    "    - United-States\n",
    "    - Cambodia\n",
    "    - England\n",
    "    - Puerto-Rico\n",
    "    - Canada\n",
    "    - Germany\n",
    "    - Outlying-US(Guam-USVI-etc)\n",
    "    - India\n",
    "    - Japan\n",
    "    - Greece\n",
    "    - South\n",
    "    - China\n",
    "    - Cuba\n",
    "    - Iran\n",
    "    - Honduras\n",
    "    - Philippines\n",
    "    - Italy\n",
    "    - Poland\n",
    "    - Jamaica\n",
    "    - Vietnam\n",
    "    - Mexico\n",
    "    - Portugal\n",
    "    - Ireland\n",
    "    - France\n",
    "    - Dominican-Republic\n",
    "    - Laos\n",
    "    - Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador,    Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "    \n",
    "    Note:\n",
    "        Class label <=50K is encode to 0,\n",
    "        Class label >50K is encode to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'input/adult_train_modified.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/2539273530.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input/adult_train_modified.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"input/adult_test_modified.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'input/adult_train_modified.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"input/adult_train_modified.csv\")\n",
    "test = pd.read_csv(\"input/adult_test_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/2887330630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#describe the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#describe the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/3692352930.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Get the shape/dimension of data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#Get the shape/dimension of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/626754103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Count observations based on attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Count observations based on attribute\n",
    "train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IR = maj class/min class\n",
    "IR>1.5 - imbalanced data\n",
    "IR>9 highly imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for null data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/1912029300.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# select rows from dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# sum of null data based on attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# select rows from dataframe\n",
    "x=train.iloc[:,:-1]\n",
    "\n",
    "# sum of null data based on attributes\n",
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 3: Data Preparation\n",
    "\n",
    "<img src=\"Part 3.jpg\" style=\"width:60%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "Data preparation is required for transformation of raw data into a form that is more suitable for modeling. In this lab, since the target label (or attribute) for the classification is categorical (class attribute = 1, 2, or 3), then *label encoder* is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the data is split into train and test sets. The train set is used to train the classifier and validate its accuracy. Then, the classifier will be evaluated by the test set to determine its performance. The following codes are used for splitting the data into train and test sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/1051261648.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# select all columns except the last one (the target label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# set target categorical data label (15th attribute)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# select all columns except the last one (the target label)\n",
    "x_train=train.iloc[:,:-1]\n",
    "# set target categorical data label (15th attribute)\n",
    "y_train=train.iloc[:,14]\n",
    "\n",
    "# select all columns except the last one (the target label)\n",
    "x_test=test.iloc[:,:-1]\n",
    "# set target categorical data label (sixth attribute)\n",
    "y_test=test.iloc[:,14]\n",
    "\n",
    "#Use line below if want to split data into training and testing\n",
    "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- x axis test ----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/2309263228.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------- x axis test ----------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------- x axis train ---------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------- y axis test ----------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "print('-------- x axis test ----------')\n",
    "print(x_test)\n",
    "print('-------- x axis train ---------')\n",
    "print(x_train)\n",
    "print('-------- y axis test ----------')\n",
    "print(y_test)\n",
    "print('-------- y axis train ---------')\n",
    "print(y_train)\n",
    "print('*******************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 4: Modelling\n",
    "\n",
    "<img src=\"Part 4.jpg\" style=\"width:60%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "### Step 5: Evaluation\n",
    "\n",
    "<img src=\"Part 5.jpg\" style=\"width:60%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "#### Machine Learning using K-nearest neighbour\n",
    "\n",
    "A simple classification task will be used to demonstrate the workings of a machine learning algorithm. There are many classification and regression algorithms that can be directly implemented using Python `sklearn` library (check this link for more: http://scikit-learn.org/stable/index.html) In this lab, the K nearest neighbour (or KNN) will be used to classify the data. \n",
    "\n",
    "Then, we apply the **KNN models** (refer to: http://scikit-learn.org/stable/modules/neighbors.html).\n",
    "\n",
    "The size of the neighborhood is controlled by the *k* parameter. For example, if set to 1, then predictions are made using the single most similar training instance to a given new pattern for which a prediction is requested. Larger data set commonly uses larger *k* value. However, larger *k* doesn't always give better result. The following code shows different accuracy results for different values of *k*. The following code shows an example of *default* application of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23828/1151161683.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Define k-value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mknn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#Estimate the accuracy of the classifier on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "# import KNN model as 'KNeighborsClassifier'\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#Define k-value\n",
    "knn=KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(x_train,y_train)\n",
    "\n",
    "#Estimate the accuracy of the classifier on test data\n",
    "y_pred=knn.predict(x_test)\n",
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result showed that the highest accuracy obtained by KNN when **k = 1 (accuracy = 0.720)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parameters of KNN includes weighting function and neighbour computing algorithm. The **weighting** function is used (`uniform` by default). which controls the way in which the training data is stored and searched. The previous example is based on *distance* measure. Its also important to consider the **algorithm** to compute the neighbours (`auto` by default). By default, `uniform` distance is used. The following are the typical weighting function and algorithm to compute the neighbours:\n",
    "\n",
    "-  **Weighting** functions used in prediction\n",
    "    -  `uniform`: all points in each neighborhood are weighted equally.\n",
    "    -  `distance`: weight points by the inverse of their distance.\n",
    "-  **Algorithm** to compute the neighbours\n",
    "    -  `ball_tree`: binary tree search in *D* dimensional hyperspheres.\n",
    "    -  `kd_tree`: binary tree search in *k* dimensional planes.\n",
    "    -  `brute`: a brute-force search.\n",
    "    -  `auto`: the most appropriate algorithm is decided based on the values passed to `fit` method.\n",
    "    \n",
    "The following is code for application of KNN with specific parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # library for plotting\n",
    "import warnings # to hide unnecesary warning\n",
    "warnings.filterwarnings('ignore')\n",
    "# line required for inline charts/plots\n",
    "%matplotlib inline\n",
    "\n",
    "# empty variable for storing the KNN metrics\n",
    "scores=[]\n",
    "\n",
    "# We try different values of k for the KNN (from k=1 up to k=20)\n",
    "lrange=list(range(1,20))\n",
    "\n",
    "# loop the KNN process\n",
    "for k in lrange:\n",
    "    # input the k value and 'distance' measure\n",
    "    knn=KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm='auto')\n",
    "    # input the train data to train KNN\n",
    "    knn.fit(x_train,y_train)\n",
    "    # see KNN prediction by inputting the test data\n",
    "    y_pred=knn.predict(x_test)\n",
    "    # append the performance metric (accuracy)\n",
    "    scores.append(metrics.accuracy_score(y_test,y_pred))\n",
    "    optimal_k = lrange[scores.index(max(scores))]\n",
    "          \n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print(\"The optimal score is %.2f\" % max(scores))\n",
    "plt.figure(2,figsize=(15,5))\n",
    "    \n",
    "# plot the results\n",
    "plt.plot(lrange, scores,ls='dashed')\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "# predict the classes of new, unseen data\n",
    "predict = knn.predict(x_test)\n",
    "\n",
    "print(\"The prediction accuracy is: {0:2.2f}{1:s}\".format(knn.score(x_test,y_test)*100,\"%\"))\n",
    "# Creates a confusion matrix\n",
    "cm = confusion_matrix(y_test, predict)\n",
    "\n",
    "\n",
    "# Transform to dataframe for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
    "                     columns = ['<=50k','>50k'])\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "ax= sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Decision Tree Accuracy:\" + str(knn.score(x_test,y_test)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result showed that the highest accuracy obtained by KNN using *distance* weighting function is when **k = 16 (accuracy = 0.79)**. Although it is *better* than the default KNN, the *accuracy* of the classifier is still **poor**.\n",
    "\n",
    "To address this, we need to conduct some *tuning* on the KNN parameter by using **cross-validation**. Obviously, the best *k* is the one that corresponds to the lowest test error rate, so let’s suppose we carry out repeated measurements of the test error for different values of *k*. Inadvertently, what we are doing is using the *test set* as a *training set*! This means that we are underestimating the true error rate since our model has been forced to fit the test set in the best possible manner. Our model is then *incapable* of generalizing to newer observations, a process known as **overfitting**. Hence, touching the test set is out of the question and must only be done at the very end of our pipeline.\n",
    "\n",
    "An alternative and smarter approach involves estimating the *test error rate* by holding out a subset of the training set from the fitting process. This subset, called the *validation* set, can be used to select the appropriate level of flexibility of our algorithm! There are different validation approaches that are used in practice, and we will be exploring one of the more popular ones called **k-fold cross validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"06.jpg\" style=\"width:60%;margin-left:auto;margin-right:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross validation (the *k* is totally unrelated to *K* of KNN) involves randomly dividing the training set into *k* groups, or folds, of approximately equal size. The first fold is treated as a *validation set*, and the method is fit on the remaining *k−1* folds. The misclassification rate is then computed on the observations in the held-out fold. This procedure is repeated *k* times; each time, a different group of observations is treated as a *validation set*. This process results in *k* estimates of the test error which are then averaged out.\n",
    "\n",
    "*Cross-validation* can be used to estimate the test error associated with a learning method in order to **evaluate** its performance, or to select the appropriate level of *flexibility*. *Scikit* learn comes in handy with its `cross_val_score()` method. We specifiy that we are performing *10* folds with the *cv=10* parameter and that our scoring metric should be **accuracy** since we are in a classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for cross validation scoring\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# empty variable for storing the KNN metrics\n",
    "scores=[]\n",
    "\n",
    "# We try different values of k for the KNN (from k=1 up to k=26)\n",
    "lrange=list(range(1,20))\n",
    "\n",
    "# loop the KNN process\n",
    "for k in lrange:\n",
    "    # input the k value and 'distance' measure\n",
    "    knn=KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm='auto')\n",
    "    # get score for the 10 fold cross validation\n",
    "    score = cross_val_score(knn, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    scores.append(score.mean())\n",
    "\n",
    "optimal_k = lrange[scores.index(max(scores))]\n",
    "          \n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print(\"The optimal score is %.2f\" % max(scores))\n",
    "\n",
    "plt.figure(2,figsize=(15,5))\n",
    "print(score)    \n",
    "# plot the results\n",
    "plt.plot(lrange, scores,ls='dashed')\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Splitting Data into Training Data and Testing Data\n",
    "\n",
    "These are two rather important concepts in *data science* and *data analysis* and are used as *tools* to prevent (or at least minimize) **overfitting/underfitting**. For example, using a statistical model like linear regression, we usually fit the model on a training set in order to make predications on a data that wasn’t trained (*general data*). \n",
    "\n",
    "**Overfitting** means that we’ve fit the model too much to the training data. This model will be very accurate on the training data but will probably be very not accurate on untrained or new data. **Underfitting** means that we’ve fit the model too little to the training data. This model does not fit the training data and therefore misses the trends in the data.\n",
    "\n",
    "<img src=\"fit.png\" style=\"width:60%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "It is worth noting the underfitting is not as prevalent as overfitting. These two problems makes the the model cannot be generalized to new data. Nevertheless, we want to avoid both of those problems in data analysis. You might say we are trying to find the middle ground between under and overfitting our model. As you will see, train/test split and cross validation help to avoid overfitting more than underfitting.\n",
    "\n",
    "As such, the data are separated into the *training* data for modelling and *test* data for validating that model. Since the algorithms are **supervised** learning algorithms, the data *label* (**target class**) and the *data* itself need to be defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "df = pd.read_csv(\"input/adult_train_modified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "Y = df.iloc[:, -1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.40, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # library for plotting\n",
    "import warnings # to hide unnecesary warning\n",
    "warnings.filterwarnings('ignore')\n",
    "# line required for inline charts/plots\n",
    "%matplotlib inline\n",
    "\n",
    "# empty variable for storing the KNN metrics\n",
    "scores=[]\n",
    "\n",
    "# We try different values of k for the KNN (from k=1 up to k=20)\n",
    "lrange=list(range(1,20))\n",
    "\n",
    "# loop the KNN process\n",
    "for k in lrange:\n",
    "    # input the k value and 'distance' measure\n",
    "    knn=KNeighborsClassifier(n_neighbors=k, weights='distance', algorithm='auto')\n",
    "    # input the train data to train KNN\n",
    "    knn.fit(x_train,y_train)\n",
    "    # see KNN prediction by inputting the test data\n",
    "    y_pred=knn.predict(x_test)\n",
    "    # append the performance metric (accuracy)\n",
    "    scores.append(metrics.accuracy_score(y_test,y_pred))\n",
    "    optimal_k = lrange[scores.index(max(scores))]\n",
    "          \n",
    "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
    "print(\"The optimal score is %.2f\" % max(scores))\n",
    "plt.figure(2,figsize=(15,5))\n",
    "    \n",
    "# plot the results\n",
    "plt.plot(lrange, scores,ls='dashed')\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
