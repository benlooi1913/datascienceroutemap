{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvRHy2MOZ_zz"
   },
   "source": [
    "# CDS503: Machine Learning\n",
    "\n",
    "***\n",
    "## LAB 5: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g70tKgatZ_z3"
   },
   "source": [
    "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nP5V9sMSZ_z7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ymHRpFgOZ_0E"
   },
   "source": [
    "## Motivating Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwkhJLSRZ_0F"
   },
   "source": [
    "We learned a simple model describing the distribution of each underlying class, and used these generative models to probabilistically determine labels for new points.\n",
    "That was an example of *generative classification*; here we will consider instead *discriminative classification*: rather than modeling each class, we simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\n",
    "\n",
    "As an example of this, consider the simple case of a classification task, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdhpBC6jZ_0I",
    "outputId": "29bb70fa-6049-4e4f-cc89-35c6d95c8088"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.samples_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27644/3907934651.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m \u001b[1;31m# library to make random data blobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# define the data that clearly seperated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_std\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# plot the data using scatter chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.samples_generator'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs # library to make random data blobs \n",
    "\n",
    "# define the data that clearly seperated\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "# plot the data using scatter chart\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiFPlwodZ_0R"
   },
   "source": [
    "A *linear discriminative* classifier would attempt to draw a straight line separating the two sets of data, and thereby create a **model** for *classification*.\n",
    "For two dimensional data like that shown here, this is a task we could do by hand.\n",
    "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
    "\n",
    "We can draw them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSL9u5OfZ_0S",
    "outputId": "49b90397-58c0-4887-916c-0be32625f2cd"
   },
   "outputs": [],
   "source": [
    "# draw a best-fit line\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "\n",
    "# plot the data using scatter chart\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "# mark the possible line point\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "# try three different data on the best-fit line\n",
    "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    # plot y = mx + b\n",
    "    plt.plot(xfit, m * xfit + b, '-k')\n",
    "\n",
    "# display within this x limits\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XGOrq-CpZ_0Z"
   },
   "source": [
    "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
    "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
    "Evidently our simple intuition of \"drawing a line between classes\" is not enough, and we need to think a bit deeper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxRgwlkAZ_0c"
   },
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines offer one way to improve on this.\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AwFNEZxZ_0f",
    "outputId": "119886f2-22e5-4b0f-9bd2-67eff65de393"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "# use three data points on the best-fit line with some margin\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    # fill the margin with color\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "# display within this x limits\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EJaeIs2Z_0n"
   },
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
    "**Support vector machines** are an example of such a *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryTwfJimZ_0o"
   },
   "source": [
    "### Fitting a support vector machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use `sklearn` support vector classifier to train an SVM model on this data.\n",
    "For the time being, we will use a `linear` kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k55k2EaPZ_0q",
    "outputId": "23d4b323-7c44-4531-cb5c-390ebd2f9cd2"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrLhoCD-Z_00"
   },
   "source": [
    "To better *visualize* what's happening here, let's create a *quick* convenience function that will plot SVM decision boundaries for us:\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXBaOd9TZ_04"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the decision function for a 2D SVC\n",
    "\"\"\"\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEe4KxNTZ_1B",
    "outputId": "4764348a-ff94-42ce-86d6-f05c1586adf2"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATV4oSYFZ_1G"
   },
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-Q3iF0qZ_1I",
    "outputId": "c4b4a4a1-17ab-455b-9582-589c3aa5eb15"
   },
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4xyzDwCZ_1Q"
   },
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfzk83dOZ_1S",
    "outputId": "de2d3895-9d7f-45b7-9921-50e8fc0d132f"
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peEyvhfAZ_1X"
   },
   "source": [
    "In the left panel, we see the model and the support vectors for 60 training points.\n",
    "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors from the left panel are still the support vectors from the right panel.\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sS3SZ5i1Z_1Z"
   },
   "source": [
    "If you are running this notebook live, you can use IPython's interactive widgets to view this feature of the SVM model interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uDSzi65IZ_1a",
    "outputId": "19958fad-c05e-4cbe-c0a7-4f2734776a67",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# library to make an interactive python notebook widget\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# plot the interactive chart\n",
    "interact(plot_svm, N=[10, 30, 50, 70, 100, 120, 150, 200], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfV9fIVPZ_1n"
   },
   "source": [
    "### Beyond linear boundaries: Kernel SVM\n",
    "\n",
    "Where **SVM** becomes *extremely* **powerful** is when it is combined with **kernels**. There we projected our data into higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to *fit* for **nonlinear** relationships with a linear classifier.\n",
    "\n",
    "In SVM models, we can use a version of the same idea.\n",
    "To motivate the need for kernels, let's look at some data that is not linearly separable:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KG0U6D2Z_1p",
    "outputId": "ac6e8800-4b95-41a5-b9c8-6efcdb94b351",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# library to generate circle data\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "# generate the data\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "# plot the chart\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "\n",
    "# try to make the best-fit line\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2204R46Z_10"
   },
   "source": [
    "It is clear that no linear discrimination will **ever** be able to *separate* this data. But we can think about how we might *project* the data into a higher dimension such that a linear separator *would* be sufficient. For example, one simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2nZUSN8Z_12"
   },
   "outputs": [],
   "source": [
    "# gaussian radial basis function\n",
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z0vCE5VZ_17"
   },
   "source": [
    "We can visualize this extra data dimension using a **three-dimensional** plot. \n",
    "\n",
    "If you are running this notebook live, you will be able to use the sliders to rotate the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PkPbRf6Z_18",
    "outputId": "2aaffd24-d439-496e-f6a2-eadf4239d4c2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "interact(plot_3D, elev=[90, 70, 50, 30, 10, -10, -30, -50, -70, -90], azip=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_TMEoNeZ_2F"
   },
   "source": [
    "We can see that with this additional dimension, the data becomes *trivially* linearly separable, by **drawing** a separating plane at, say, *r*=0.7.\n",
    "\n",
    "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In `sklearn`, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vi9R2NUuZ_2Q",
    "outputId": "6e5232e5-5586-429c-f4ed-a06f10fd067b"
   },
   "outputs": [],
   "source": [
    "# change the kernel from \"linear\" to \"rbf\"\n",
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s19EHfjkZ_2U"
   },
   "source": [
    "Using this **kernelized** support vector machine, we learn a *suitable* nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the *kernel trick* can be used.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHpf2-XUZ_2W"
   },
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has *centered* around very **clean datasets**, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap? For example, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fyiZwElDZ_2W",
    "outputId": "4acc6c7e-499f-451f-efa2-22ea40836ebe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OioffoORZ_2Z"
   },
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
    "For very large $C$, the margin is hard, and points cannot lie in it.\n",
    "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a **changing** $C$ parameter affects the *final fit*, via the **softening** of the *margin*:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTCsAO4-Z_2Z",
    "outputId": "f525722a-a463-49cb-86c2-4df9b715710a"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "# test two different 'c' values (10 and 0.1) and plot the results\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar2Yetx_Z_2g"
   },
   "source": [
    "The **optimal** value of the $C$ parameter will depend on your dataset, and should be **tuned** using cross-validation or a similar procedure.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jy01kmiuZ_2h"
   },
   "source": [
    "***\n",
    "### Step1: Business Understanding\n",
    "\n",
    "***\n",
    "\n",
    "This data was extracted from the census bureau database found at: http://www.census.gov/ftp/pub/DES/www/welcome.html. It contains the cencus income of the people. They are trying to see the income of more than 50k and less than 50k.\n",
    "| Class label '>50K'  : 23.93% / 24.78% (without unknowns)\n",
    "| Class label '<=50K' : 76.07% / 75.22% (without unknowns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3jqluQAZ_2i",
    "outputId": "a37e4b04-8957-4aa2-b808-17f771b9104c",
    "scrolled": true
   },
   "source": [
    "<img src=\"Part 2.png\" style=\"width:80%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "### Step 2: Data Understanding\n",
    "\n",
    "#### Description of the data:\n",
    "In the censuc income data set, there are fifteen attributes including the class attribute indicating the class/category information. \n",
    "The 15 attributes are:\n",
    "-  age: continuous.\n",
    "-  workclass:\n",
    "    - Private\n",
    "    - Self-emp-not-inc\n",
    "    - Self-emp-inc\n",
    "    - Federal-gov\n",
    "    - Local-gov\n",
    "    - State-gov\n",
    "    - Without-pay\n",
    "    - Never-worked\n",
    "- fnlwgt: continuous.\n",
    "- education: \n",
    "    - Bachelors \n",
    "    - Some-college\n",
    "    - 11th\n",
    "    - HS-grad\n",
    "    - Prof-school\n",
    "    - Assoc-acdm\n",
    "    - Assoc-voc\n",
    "    - 9th\n",
    "    - 7th-8th\n",
    "    - 12th\n",
    "    - Masters\n",
    "    - 1st-4th\n",
    "    - 10th\n",
    "    - Doctorate\n",
    "    - 5th-6th\n",
    "    - Preschool\n",
    "- education-num: continuous.\n",
    "- marital-status: \n",
    "    - Married-civ-spouse\n",
    "    - Divorced\n",
    "    - Never-married\n",
    "    - Separated\n",
    "    - Widowed\n",
    "    - Married-spouse-absent\n",
    "    - Married-AF-spouse\n",
    "- occupation: \n",
    "    - Tech-support\n",
    "    - Craft-repair\n",
    "    - Other-service\n",
    "    - Sales\n",
    "    - Exec-managerial\n",
    "    - Prof-specialty\n",
    "    - Handlers-cleaners\n",
    "    - Machine-op-inspct\n",
    "    - Adm-clerical\n",
    "    - Farming-fishing\n",
    "    - Transport-moving\n",
    "    - Priv-house-serv\n",
    "    - Protective-serv\n",
    "    - Armed-Forces\n",
    "- relationship: \n",
    "    - Wife\n",
    "    - Own-child\n",
    "    - Husband\n",
    "    - Not-in-family\n",
    "    - Other-relative\n",
    "    - Unmarried\n",
    "- race: \n",
    "    - White\n",
    "    - Asian-Pac-Islander\n",
    "    - Amer-Indian-Eskimo\n",
    "    - Other\n",
    "    - Black\n",
    "- sex: \n",
    "    - Female\n",
    "    - Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous.\n",
    "- native-country: \n",
    "    - United-States\n",
    "    - Cambodia\n",
    "    - England\n",
    "    - Puerto-Rico\n",
    "    - Canada\n",
    "    - Germany\n",
    "    - Outlying-US(Guam-USVI-etc)\n",
    "    - India\n",
    "    - Japan\n",
    "    - Greece\n",
    "    - South\n",
    "    - China\n",
    "    - Cuba\n",
    "    - Iran\n",
    "    - Honduras\n",
    "    - Philippines\n",
    "    - Italy\n",
    "    - Poland\n",
    "    - Jamaica\n",
    "    - Vietnam\n",
    "    - Mexico\n",
    "    - Portugal\n",
    "    - Ireland\n",
    "    - France\n",
    "    - Dominican-Republic\n",
    "    - Laos\n",
    "    - Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador,    Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UyBvcXx7Z_2p",
    "outputId": "db57e229-c8a3-477d-ec16-445db7791d3d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import warnings # to hide unnecesary warning\n",
    "warnings.filterwarnings('ignore')\n",
    "                                                            \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library to display multiple outputs\n",
    "from IPython.display import display\n",
    "\n",
    "# Importing dataset\n",
    "train = pd.read_csv(\"input/adult_train_modified.csv\")\n",
    "test = pd.read_csv(\"input/adult_test_modified.csv\")\n",
    "\n",
    "# see some of it, their overall statistics and dimensions\n",
    "display(train.head(5))\n",
    "display(train.describe())\n",
    "display(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#describe the data\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the shape/dimension of data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count observations based on attribute\n",
    "train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select rows from dataframe\n",
    "x=train.iloc[:,:-1]\n",
    "\n",
    "# sum of null data based on attributes\n",
    "x.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Step 4: Modelling\n",
    "#### Classify using Support Vector Machine\n",
    "\n",
    "Here, the next **question** directly arises: \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"Part 4.png\" style=\"width:80%;margin-left:auto;margin-right:auto;\">\n",
    "\n",
    "### Step 5: Evaluation\n",
    "\n",
    "<img src=\"Part 5.png\" style=\"width:80%;margin-left:auto;margin-right:auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns except the last one (the target label)\n",
    "x_train=train.iloc[:,:-1]\n",
    "# set target categorical data label (sixth attribute)\n",
    "y_train=train.iloc[:,14]\n",
    "\n",
    "# select all columns except the last one (the target label)\n",
    "x_test=test.iloc[:,:-1]\n",
    "# set target categorical data label (sixth attribute)\n",
    "y_test=test.iloc[:,14]\n",
    "\n",
    "#Use line below if want to split data into training and testing\n",
    "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.4,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------- x axis test ----------')\n",
    "print(x_test)\n",
    "print('-------- x axis train ---------')\n",
    "print(x_train)\n",
    "print('-------- y axis test ----------')\n",
    "print(y_test)\n",
    "print('-------- y axis train ---------')\n",
    "print(y_train)\n",
    "print('*******************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-Max scaler\n",
    "Transform features by scaling each feature to a given range. This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g., between zero and one. This Scaler shrinks the data within the range of -1 to 1 if there are negative values. We can set the range like [0,1] or [0,5] or [-1,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaling = MinMaxScaler(feature_range=(-1,1)).fit(x_train)\n",
    "x_train = scaling.transform(x_train)\n",
    "x_test = scaling.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_linear= SVC(C=1, kernel='linear', gamma = 0.001)\n",
    "svc_linear.fit(x_train, y_train)\n",
    "yfit = svc_linear.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification report metrices\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, yfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27644/3286171059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# compute the confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myfit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# Transform to dataframe for easier plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# import the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, yfit)\n",
    "# Transform to dataframe for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
    "                     columns = ['<=50k','>50k'])\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "ax= sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Support Vector Machine Accuracy:\" + str(svc_linear.score(x_test,y_test)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use a `grid search` cross-validation to explore **combinations** of parameters.\n",
    "Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` , and determine the best model:\n",
    "***\n",
    "Other parameters in SVM:\n",
    "class sklearn.svm.SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=- 1, decision_function_shape='ovr', break_ties=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svc_linear' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27644/959922994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m param_grid= {'C': [1, 5, 10, 50],\n\u001b[0;32m      9\u001b[0m               'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc_linear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'svc_linear' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "param_grid= {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(svc_linear, param_grid, verbose=1, n_jobs=-1)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "gridSVM=grid.best_params_\n",
    "print(gridSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = grid.best_estimator_\n",
    "svcGrid= SVC(C=5, kernel='linear', gamma = 0.0001)\n",
    "svcGrid.fit(x_train, y_train)\n",
    "yfitGrid = svcGrid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification report metrices\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, yfitGrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, yfitGrid)\n",
    "# Transform to dataframe for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
    "                     columns = ['<=50k','>50k'])\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "ax= sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Support Vector Machine Accuracy:\" + str(svcGrid.score(x_test,y_test)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "\n",
    "Let's change our kernel to rbf now and observed the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_rbf= SVC(C=1, kernel='rbf', gamma = 0.001)\n",
    "svc_rbf.fit(x_train, y_train)\n",
    "yfit = svc_rbf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification report metrices\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, yfit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, yfit)\n",
    "# Transform to dataframe for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
    "                     columns = ['<=50k','>50k'])\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "ax= sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Support Vector Machine Accuracy:\" + str(svc_rbf.score(x_test,y_test)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "param_grid= {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(svc_rbf, param_grid, verbose=1, n_jobs=-1)\n",
    "grid.fit(x_train, y_train)\n",
    "\n",
    "gridSVM=grid.best_params_\n",
    "print(gridSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = grid.best_estimator_\n",
    "svcGridRBF= SVC(C=50, kernel='rbf', gamma = 0.005)\n",
    "svcGridRBF.fit(x_train, y_train)\n",
    "yfitGridrbf = svcGridRBF.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import classification report metrices\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, yfitGridrbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, yfitGridrbf)\n",
    "# Transform to dataframe for easier plotting\n",
    "cm_df = pd.DataFrame(cm, index = ['<=50k','>50k'], \n",
    "                     columns = ['<=50k','>50k'])\n",
    "\n",
    "# plot the confusion matrix\n",
    "plt.figure(figsize=(8,8))\n",
    "ax= sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.title(\"Support Vector Machine Accuracy:\" + str(svcGridRBF.score(x_test,y_test)*100))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Summary\n",
    "\n",
    "We have seen here a brief intuitive introduction to the principals behind support vector machines.\n",
    "These methods are a powerful classification method for a number of reasons:\n",
    "\n",
    "- Their dependence on **relatively few support vectors** means that they are **very compact** models, and take up very little memory.\n",
    "- Once the model is **trained**, the prediction phase is *very fast*.\n",
    "- Because they are affected only by points near the margin, they **work well with high-dimensional** data; even data with more dimensions than samples, which is a challenging regime for other algorithms.\n",
    "- Their *integration* with **kernel methods** makes them very *versatile*, able to **adapt** to many types of data.\n",
    "\n",
    "However, SVMs have several disadvantages as well:\n",
    "\n",
    "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. This mean that for **large numbers** of training samples, the computational cost can be prohibitive (very long, even can took days or weeks!).\n",
    "- The results are **strongly dependent** on a *suitable choice* for the softening parameter $C$. This must be **carefully chosen** via cross-validation, which can be expensive as datasets grow in size.\n",
    "\n",
    "With those traits in mind, SVMs generally used when other simpler, faster, and less tuning-intensive methods have been shown to be **insufficient** for your needs. Nevertheless, if you have the CPU cycles to **commit** to *training* and *cross-validating* an SVM on your data, the method can lead to **excellent** results.\n",
    "***\n",
    "\n",
    "Additional link for SVM parameter exploration:\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "05.07-Support-Vector-Machines.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
